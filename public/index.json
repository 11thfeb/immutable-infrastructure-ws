[
{
	"uri": "//localhost:1313/",
	"title": "Packer Ansible Terraform for Immutable Infrastructure",
	"tags": [],
	"description": "",
	"content": "Packer/Ansible/Terraform for Immutable Infrastructure Overall In this workshop, we\u0026rsquo;ll explore the fundamentals and practical exercises of Packer, Ansible, and Terraform for Immutable Infrastructure. These tools are essential for automating your infrastructure provisioning and deployment processes. You\u0026rsquo;ll gain hands-on experience by creating connections for both public and private instances, enabling you to streamline your development workflow effectively. Let\u0026rsquo;s dive in and discover the capabilities of Packer, Ansible, and Terraform together\nContent Introduction Preparation Create user, policy \u0026amp; access key Configure AWS CLI Write Ansible Playbook Write Packer Write Terraform Clean up resources "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.1-install-packer/",
	"title": "Installing Packer",
	"tags": [],
	"description": "",
	"content": "2.1. Installing Packer https://developer.hashicorp.com/packer/install?product_intent=packer\n$ wget -O- https://apt.releases.hashicorp.com/gpg | sudo gpg --dearmor -o /usr/share/keyrings/hashicorp-archive-keyring.gpg $ echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; | sudo tee /etc/apt/sources.list.d/hashicorp.list $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install packer Verifying the Installation After installing Packer, verify the installation worked by opening a new command prompt or console, and checking that packer is available:\n$ packer --version Packer v1.10.3 "
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Why Immutable Servers? The DevOps community has embraced the immutable server concept, with good reason. Immutable servers are fully provisioned by automated tooling, and an engineer never needs to access the server to configure it manually. This results in predictable systems – the way a server is configured can be predicted near-exactly by what is in your source control. Immutable servers are faster to deploy, because computers can type more quickly than people.\nThey can also be more secure. When servers are predictable and fast to re-deploy, sysadmins don’t have much incentive to leave them running for long periods of time. If you’re in the habit of re-provisioning your servers daily, (or even hourly, in the case of some companies who deploy multiple times a day) then an intruder who does make it onto your machine won’t have as much time to set up shop and explore. Long running servers are risky: the Equifax breach occurred over a period of 76 days. Immutable servers with a short lifespan are a great addition to a strong defense in depth strategy.\nWhat are the DevOps tools? There are many approaches to creating immutable servers. When I joined my current project at company, I encountered a new-to-me stack used to create immutable servers in the Amazon Web Services (AWS) GovCloud. While familiar with Terraform, I hadn’t used either Packer or Ansible before. Now that I’ve spent the good part of the year working with this stack, I’m a convert. While the stack is a bit complex to setup (at least compared to the good old “bash script that runs on new nodes” paradigm) the reliability, fast deployment speeds, and visibility afforded by this stack make the initial pain of setup well worth it.\nPacker https://www.packer.io\nPacker is a Hashicorp product. In their words, “Packer is an open source tool for creating identical machine images for multiple platforms from a single source configuration.” We use Packer to take US Government approved Amazon Machine Images (AMIs) running Red Hat 7 and produce new versions of these AMIs that have all the configuration and software we need to run our application securely in AWS.\nPacker supports multiple “provisioners,” which handle the actual server configuration. These can be simple shell scripts, or can be a more robust tool like Ansible. Packer handles the creation of the VM and packaging as an AMI, Ansible handles the configuration of the virtual machine.\nAnsible https://www.ansible.com\nAnsible was created by Red Hat as a configuration management tool. It automates all software installation, package management, and configuration on our AWS EC2 hosts. Ansible ensures that any software, config file change, or cron job is installed the same way every time.\nAnsible is agentless, so your build toolchain has fewer moving parts. (Probably the worst part of DevOps is becoming a sysadmin for the tools you build to replace you as a sysadmin.) The YAML syntax is incredible and well-documented. The Ansible Galaxy ecosystem is well-stocked with playbooks for all kinds of server management tasks, from deploying an Apache server to complying with federal system security guidelines.\nTerraform https://www.terraform.io/\nTerraform is a Hashicorp product. It allows us to “safely and predictably create, change, and improve infrastructure.” The term infrastructure refers to components like EC2 instances, load balancers, databases, and networking. Terraform integrates with AWS APIs to translate and run configuration code into AWS API calls that provision our architecture – everything from the networking to the app servers and S3 buckets. Once we’ve “baked” an AMI using Packer and Ansible, we use Terraform to deploy that AMI as an EC2 instance into our cloud environment. This results in a live server in under 3 minutes.\n"
},
{
	"uri": "//localhost:1313/2-prerequiste/2.2-install-ansible/",
	"title": "Installing Ansible",
	"tags": [],
	"description": "",
	"content": "2.2. Installing Ansible https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html\n# Install Ansible repository. $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y $ sudo apt-get install -y software-properties-common $ sudo apt-add-repository -y ppa:ansible/ansible # Install Ansible. $ sudo apt-get update $ sudo apt-get install -y ansible Verifying the Installation After installing Ansible, verify the installation worked by opening a new command prompt or console, and checking that ansible is available:\n$ ansible --version ansible [core 2.14.6] "
},
{
	"uri": "//localhost:1313/2-prerequiste/",
	"title": "Preparation ",
	"tags": [],
	"description": "",
	"content": "\rNotes: We need to start and enable nginx using systemctl start and enable to ensure that nginx will be ready to process spontaneously.\nIn this workshop, I will be baking an AMI using Packer and doing configuration using Ansible during the baking process. A static website will be deployed. And the same Ansible code is being used to provision the static website using Nginx during this whole baking process. Tags will be assigned to AMI during baking process. It is intended to identify the latest AMI available and have it ready for EC2 instance creation. Subnet ID is supposed to be collected for Packer builder so that it is able to create AMI using the Subnet ID. Here we are going to manage our terraform code in two different sections: one is for creating VPC, subnet, and other network information, the other is for grouping EC2 instance inside our network using AMI created by the Packer previously.\nContent Installing Packer Installing Ansible Installing AWS CLI Installing Terraform "
},
{
	"uri": "//localhost:1313/3-create-user-policy/",
	"title": "Create user, policy &amp; access key",
	"tags": [],
	"description": "",
	"content": "In this step, we will create a user for this lab\nCreate user, policy \u0026amp; access key. Go to IAM service management console, click “Create user” button to create your user. Input your user name, and click on the “Next” button to proceed to the next step. Choose “Attach policies directly”, after that you can search in the search bar “AdministratorAccess” and “EC2FullAccess”, choose it and click on the “Next” button to proceed to the next step. You can see the configuration for your account in the review tag, click on the “Create user” button to create your user. Go to Access Management console Click on the “Create access key” button to create your access key. Click on the “Command Line Interface (CLI)”. Choose “I understand the above recommendation and want to proceed to create an access key” checkbox and “Next” button to proceed to the next step. Input your access key name and click “Create access key” button to create your access key. You can see your acesskey and secret key in the screen. You should save your access key and secret key for the next step. "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.3-install-aws-cli/",
	"title": "Installing AWS CLI",
	"tags": [],
	"description": "",
	"content": "2.3. Installing AWS CLI https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html\n$ sudo apt-get update $ sudo apt-get install python3-pip $ sudo pip install awscli Verifying the Installation After installing AWS CLI, verify the installation worked by opening a new command prompt or console, and checking that AWS CLI is available:\n## Verify Installation $ aws --version "
},
{
	"uri": "//localhost:1313/4-configure-aws-cli/",
	"title": "Configure AWS CLI",
	"tags": [],
	"description": "",
	"content": "In the previous step, you created an account and obtained an access key and secret key. In this step, you need to use the access key and secret key to configure the AWS CLI.\nFire up your CLI and enter the following command:\n$ aws configure AWS Access Key ID [********************]: AWS Secret Access Key [********************]: Default region name [us-east-1]: Default output format [None]: You will be prompted to enter your AWS Access Key ID, AWS Secret Access Key, Default region name, and Default output format.\nAfter you have successfully configured AWS CLI, you can start using it to manage your AWS services.\nCommands Frequently Used in AWS CLI. To describe the EC2 instance. aws ec2 describe-instances To list all the IAM user. aws iam list-users "
},
{
	"uri": "//localhost:1313/2-prerequiste/2.4-install-terraform/",
	"title": "Install Terraform",
	"tags": [],
	"description": "",
	"content": "2.3. Install Terraform https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli\nEnsure that your system is up to date and you have installed the gnupg, software-properties-common, and curl packages installed. You will use these packages to verify HashiCorp\u0026rsquo;s GPG signature and install HashiCorp\u0026rsquo;s Debian package repository.\n$ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y gnupg software-properties-common Install the HashiCorp GPG key.\n$ wget -O- https://apt.releases.hashicorp.com/gpg | \\ gpg --dearmor | \\ sudo tee /usr/share/keyrings/hashicorp-archive-keyring.gpg \u0026gt; /dev/null Verify the key\u0026rsquo;s fingerprint.\ngpg --no-default-keyring \\ --keyring /usr/share/keyrings/hashicorp-archive-keyring.gpg \\ --fingerprint The gpg command will report the key fingerprint:\n/usr/share/keyrings/hashicorp-archive-keyring.gpg ------------------------------------------------- pub rsa4096 XXXX-XX-XX [SC] AAAA AAAA AAAA AAAA uid [ unknown] HashiCorp Security (HashiCorp Package Signing) \u0026lt;security+packaging@hashicorp.com\u0026gt; sub rsa4096 XXXX-XX-XX [E] Add the official HashiCorp repository to your system. The lsb_release -cs command finds the distribution release codename for your current system, such as buster, groovy, or sid.\n$ echo \u0026#34;deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg] \\ https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/hashicorp.list Download \u0026amp; Install Terraform from the new repository.\n$ sudo apt update $ sudo apt-get install terraform Verifying the Installation After installing Terraform, verify the installation worked by opening a new command prompt or console, and checking that Terraform is available:\n$ terraform -help Usage: terraform [-version] [-help] \u0026lt;command\u0026gt; [args] The available commands for execution are listed below. The most common, useful commands are shown first, followed by less common or more advanced commands. If you\u0026#39;re just getting started with Terraform, stick with the common commands. For the other commands, please read the help and docs before usage. #... "
},
{
	"uri": "//localhost:1313/5-ansible/",
	"title": "Write Ansible Playbook",
	"tags": [],
	"description": "",
	"content": "First, let\u0026rsquo;s create the Ansible directory structure as shown below.\n$ tree . ├── ansible │ ├── ansible-playbook-wordpress-nginx │ │ ├── README.md │ │ ├── group_vars │ │ │ └── all │ │ ├── hosts │ │ ├── playbook.retry │ │ ├── playbook.yml │ │ └── roles │ │ ├── nginx │ │ │ ├── handlers │ │ │ │ └── main.yml │ │ │ ├── tasks │ │ │ │ └── main.yml │ │ │ └── templates │ │ │ ├── default-site.conf │ │ │ └── nginx-wp-common.conf │ │ ├── php │ │ │ ├── handlers │ │ │ │ └── main.yml │ │ │ └── tasks │ │ │ └── main.yml │ │ └── wordpress │ │ ├── README.md │ │ ├── handlers │ │ │ └── main.yml │ │ ├── tasks │ │ │ └── main.yml │ │ └── templates │ │ └── wp-config.php In the group_vars/all, enter the following content:\n## group_vars/all # WordPress database settings wp_db_name: wordpress wp_db_user: wordpress wp_db_password: wordpress_db_password mysql_port: 3306 mysql_root_password: root auto_up_disable: true server_hostname: 11thfeb # WordPress Version wp_version: 4.7.5 wp_sha1sum: fbe0ee1d9010265be200fe50b86f341587187302 #Define Core Update Level #true = Development, minor, and major updates are all enabled #false = Development, minor, and major updates are all disabled #minor = Minor updates are enabled, development, and major updates are disabled core_update_level: true In the host, enter the following content:\n[webservers] localhost In the playbook.retry, enter the following content:\nlocalhost In the playbook.yml, enter the following content:\n- hosts: localhost roles: - nginx - wordpress - php Ansible Roles provide a structured way to organize tasks, templates, files, and variables. This structure makes it easier to manage complex automation setups, as everything related to a specific role is contained within its directory.\nIn the roles/nginx folder, let’s get start write configure for task and handlers.\nCreate a file main.yml in roles/nginx/handlers:\n--- - name: restart nginx service: name=nginx state=restarted become: yes Create a file main.yml in roles/nginx/tasks:\n--- # tasks file for nginx - name: Update apt cache apt: update_cache=yes cache_valid_time=3600 become: yes - name: Install nginx apt: name={{ item }} state=present become: yes with_items: - nginx - git - name: Start nginx become: yes service: name: nginx state: started - name: Update nginx confs for WordPress + PHP template: \u0026#34;src=../templates/default-site.conf dest=/etc/nginx/sites-available/default owner=www-data group=www-data mode=0644\u0026#34; become: yes - name: Enable site file: src=/etc/nginx/sites-available/default dest=/etc/nginx/sites-enabled/default owner=www-data group=www-data state=link notify: - restart nginx become: yes You need to create two file in roles/nginx/templates:\ndefault-site.conf server { listen 80 default_server; listen [::]:80 default_server; root /var/www/11thfeb; # Add index.php to the list if you are using PHP index index.html index.nginx-debian.html readme.html; server_name _; location / { # First attempt to serve request as file, then # as directory, then fall back to displaying a 404. #try_files $uri $uri/ =404; try_files $uri $uri/ /index.php$is_args$args; } # include /etc/nginx/nginx-wp-common.conf; } nginx-wp-common.conf charset utf-8; location / { index index.php index.html readme.html; try_files $uri $uri/ /index.php?$args; } error_page 404 /404.html; error_page 500 502 503 504 /50x.html; location = /50x.html { root /usr/share/nginx/html; } # Disallow .php files in uploads location ~* /(?:uploads|files)/.*\\.php$ { deny all; } # Add trailing slash to */wp-admin requests rewrite /wp-admin$ $scheme://$host$uri/ permanent; # Prevent hidden files (beginning with a period) from being served location ~ /\\. { access_log off; log_not_found off; deny all; } # Pass uploaded files to wp-includes/ms-files.php rewrite /files/$ /index.php last; if ($uri !~ wp-content/plugins) { rewrite /files/(.+)$ /wp-includes/ms-files.php?file=$1 last; } # Rewrite multisite in a subdirectory \u0026#39;.../wp-.*\u0026#39; and \u0026#39;.../*.php\u0026#39; if (!-e $request_filename) { rewrite ^/[_0-9a-zA-Z-]+(/wp-.*) $1 last; rewrite ^/[_0-9a-zA-Z-]+.*(/wp-admin/.*\\.php)$ $1 last; rewrite ^/[_0-9a-zA-Z-]+(/.*\\.php)$ $1 last; } location ~ \\.php$ { try_files $uri =404; include /etc/nginx/fastcgi_params; fastcgi_read_timeout 3600s; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_pass unix:/run/php/php7.0-fpm.sock; fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_index index.php; } Continue configuration to download and install wordpress.\nCreate a file main.yml in roles/wordpress/handlers:\n--- - name: restart nginx service: name=nginx state=restarted become: yes - name: restart php5-fpm service: name=php5-fpm state=restarted Continue configuration to download and install PHP.\nCreate a file main.yml in roles/php/handlers:\n--- - name: restart nginx service: name=nginx state=restarted become: yes Create a file main.yml in roles/php/tasks:\n--- # tasks file for php - name: Update apt cache apt: update_cache=yes cache_valid_time=3600 become: yes - name: Add ppa Repository become: yes apt_repository: repo=ppa:ondrej/php - name: Install php extensions apt: name: \u0026#34;{{ item }}\u0026#34; state: latest become: yes with_items: - php7.0 - php7.0-mysql - php7.0-gd - php7.0-mcrypt - name: Setup php-fpm replace: dest=/etc/php/7.0/cli/php.ini regexp=\u0026#34;(;cgi.fix_pathinfo=1)\u0026#34; replace=\u0026#34;cgi.fix_pathinfo=0\u0026#34; notify: - restart nginx become: yes - name: Add php settings template: src=../../nginx/templates/nginx-wp-common.conf dest=/etc/nginx/nginx-wp-common.conf owner=www-data group=www-data mode=0644 notify: - restart nginx become: yes - name: Remove apache2 become: true apt: name: apache2 state: absent "
},
{
	"uri": "//localhost:1313/6-packer/",
	"title": "Write Packer",
	"tags": [],
	"description": "",
	"content": "First, let\u0026rsquo;s create the Packer directory structure as shown below.\n$ tree ├── ansible │ ├── ansible-playbook-wordpress-nginx |......... ├── packer │ ├── scripts │ │ ├── ansible.sh │ ├── template.pkr.hcl Create a file ansible.sh in scripts folder:\n#!/bin/bash -eux # Install Ansible repository. sudo apt-get update \u0026amp;\u0026amp; sudo apt-get upgrade -y sudo apt-get install -y software-properties-common sudo apt-add-repository -y ppa:ansible/ansible # Install Ansible. sudo apt-get update sudo apt-get install -y ansible Create a file template.pkr.hcl in packer folder:\npacker { required_plugins { amazon = { version = \u0026#34;\u0026gt;= 1.2.8\u0026#34; source = \u0026#34;github.com/hashicorp/amazon\u0026#34; } } required_plugins { ansible = { version = \u0026#34;~\u0026gt; 1\u0026#34; source = \u0026#34;github.com/hashicorp/ansible\u0026#34; } } } source \u0026#34;amazon-ebs\u0026#34; \u0026#34;ubuntu\u0026#34; { ami_name = \u0026#34;your-ami-name\u0026#34; force_deregister = true instance_type = \u0026#34;t2.micro\u0026#34; region = \u0026#34;us-east-1\u0026#34; source_ami_filter { filters = { name = \u0026#34;ubuntu/images/*ubuntu-jammy-22.04-amd64-server-*\u0026#34;, root-device-type = \u0026#34;ebs\u0026#34; virtualization-type = \u0026#34;hvm\u0026#34; } most_recent = true owners = [\u0026#34;099720109477\u0026#34;] } ssh_username = \u0026#34;ubuntu\u0026#34; } build { name = \u0026#34;your-ami-name\u0026#34; sources = [\u0026#34;source.amazon-ebs.ubuntu\u0026#34;] provisioner \u0026#34;shell\u0026#34; { script = \u0026#34;scripts/ansible.sh\u0026#34; } provisioner \u0026#34;ansible-local\u0026#34; { group_vars = \u0026#34;../ansible/ansible-playbook-wordpress-nginx/group_vars\u0026#34; playbook_file = \u0026#34;../ansible/ansible-playbook-wordpress-nginx/playbook.yml\u0026#34; role_paths = [ \u0026#34;../ansible/ansible-playbook-wordpress-nginx/roles/nginx\u0026#34;, \u0026#34;../ansible/ansible-playbook-wordpress-nginx/roles/wordpress\u0026#34;, \u0026#34;../ansible/ansible-playbook-wordpress-nginx/roles/php\u0026#34; ] } } After you have finished coding according to the template above, you can run the following command.\n## Check that a template is valid $ packer validate ## Install missing plugins or upgrade plugins $ packer init ## Build image(s) from template $ packer build template.pkr.hcl While waiting for Packer to build the image, you can go and make a cup of coffee while you wait.\nGo to Amazon Machine Images (AMIs), you can see your image created.\n"
},
{
	"uri": "//localhost:1313/7-terraform/",
	"title": "Write Terraform",
	"tags": [],
	"description": "",
	"content": "First, let\u0026rsquo;s create the Terraform directory structure as shown below.\n$ tree . ├── modules │ └── services │ └── webserver-cluster │ ├── main.tf │ ├── outputs.tf │ └── variables.tf ├── prod │ ├── data-stores │ │ └── mysql │ │ ├── main.tf │ │ ├── outputs.tf │ │ └── variables.tf │ └── services │ └── webserver-cluster │ ├── main.tf │ ├── outputs.tf │ └── variables.tf Create a file main.tf in /modules/services/webserver-cluster directory: terraform { required_version = \u0026#34;\u0026gt;= 1.0.0, \u0026lt; 2.0.0\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } resource \u0026#34;aws_launch_configuration\u0026#34; \u0026#34;example\u0026#34; { image_id = var.ami instance_type = var.instance_type security_groups = [aws_security_group.instance.id] # Required when using a launch configuration with an auto scaling group. lifecycle { create_before_destroy = true } } resource \u0026#34;aws_autoscaling_group\u0026#34; \u0026#34;example\u0026#34; { launch_configuration = aws_launch_configuration.example.name vpc_zone_identifier = data.aws_subnets.default.ids target_group_arns = [aws_lb_target_group.asg.arn] health_check_type = \u0026#34;ELB\u0026#34; min_size = var.min_size max_size = var.max_size tag { key = \u0026#34;Name\u0026#34; value = var.cluster_name propagate_at_launch = true } } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;allow_server_http_inbound\u0026#34; { type = \u0026#34;ingress\u0026#34; security_group_id = aws_security_group.instance.id from_port = var.server_port to_port = var.server_port protocol = local.tcp_protocol cidr_blocks = local.all_ips } resource \u0026#34;aws_lb\u0026#34; \u0026#34;example\u0026#34; { name = var.cluster_name load_balancer_type = \u0026#34;application\u0026#34; subnets = data.aws_subnets.default.ids security_groups = [aws_security_group.alb.id] } resource \u0026#34;aws_lb_listener\u0026#34; \u0026#34;http\u0026#34; { load_balancer_arn = aws_lb.example.arn port = local.http_port protocol = \u0026#34;HTTP\u0026#34; # By default, return a simple 404 page default_action { type = \u0026#34;fixed-response\u0026#34; fixed_response { content_type = \u0026#34;text/plain\u0026#34; message_body = \u0026#34;404: page not found\u0026#34; status_code = 404 } } } resource \u0026#34;aws_lb_target_group\u0026#34; \u0026#34;asg\u0026#34; { name = var.cluster_name port = var.server_port protocol = \u0026#34;HTTP\u0026#34; vpc_id = data.aws_vpc.default.id health_check { path = \u0026#34;/\u0026#34; protocol = \u0026#34;HTTP\u0026#34; matcher = \u0026#34;200\u0026#34; interval = 15 timeout = 3 healthy_threshold = 2 unhealthy_threshold = 2 } } resource \u0026#34;aws_lb_listener_rule\u0026#34; \u0026#34;asg\u0026#34; { listener_arn = aws_lb_listener.http.arn priority = 100 condition { path_pattern { values = [\u0026#34;*\u0026#34;] } } action { type = \u0026#34;forward\u0026#34; target_group_arn = aws_lb_target_group.asg.arn } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;alb\u0026#34; { name = \u0026#34;${var.cluster_name}-alb\u0026#34; } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;allow_http_inbound\u0026#34; { type = \u0026#34;ingress\u0026#34; security_group_id = aws_security_group.alb.id from_port = local.http_port to_port = local.http_port protocol = local.tcp_protocol cidr_blocks = local.all_ips } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;allow_all_outbound\u0026#34; { type = \u0026#34;egress\u0026#34; security_group_id = aws_security_group.alb.id from_port = local.any_port to_port = local.any_port protocol = local.any_protocol cidr_blocks = local.all_ips } data \u0026#34;terraform_remote_state\u0026#34; \u0026#34;db\u0026#34; { backend = \u0026#34;s3\u0026#34; config = { bucket = var.db_remote_state_bucket key = var.db_remote_state_key region = \u0026#34;us-east-1\u0026#34; } } locals { http_port = 80 any_port = 0 any_protocol = \u0026#34;-1\u0026#34; tcp_protocol = \u0026#34;tcp\u0026#34; all_ips = [\u0026#34;0.0.0.0/0\u0026#34;] } data \u0026#34;aws_vpc\u0026#34; \u0026#34;default\u0026#34; { default = true } data \u0026#34;aws_subnets\u0026#34; \u0026#34;default\u0026#34; { filter { name = \u0026#34;vpc-id\u0026#34; values = [data.aws_vpc.default.id] } } Create a file variables.tf in /modules/services/webserver-cluster directory: # --------------------------------------------------------------------------------------------------------------------- # REQUIRED PARAMETERS # You must provide a value for each of these parameters. # --------------------------------------------------------------------------------------------------------------------- variable \u0026#34;ami\u0026#34; { description = \u0026#34;ami\u0026#34; type = string default = \u0026#34;your ami created previous step\u0026#34; } variable \u0026#34;cluster_name\u0026#34; { description = \u0026#34;The name to use for all the cluster resources\u0026#34; type = string } variable \u0026#34;db_remote_state_bucket\u0026#34; { description = \u0026#34;The name of the S3 bucket for the database\u0026#39;s remote state\u0026#34; type = string } variable \u0026#34;db_remote_state_key\u0026#34; { description = \u0026#34;The path for the database\u0026#39;s remote state in S3\u0026#34; type = string } variable \u0026#34;instance_type\u0026#34; { description = \u0026#34;The type of EC2 Instances to run (e.g. t2.micro)\u0026#34; type = string } variable \u0026#34;min_size\u0026#34; { description = \u0026#34;The minimum number of EC2 Instances in the ASG\u0026#34; type = number } variable \u0026#34;max_size\u0026#34; { description = \u0026#34;The maximum number of EC2 Instances in the ASG\u0026#34; type = number } variable \u0026#34;server_port\u0026#34; { description = \u0026#34;The port the server will use for HTTP requests\u0026#34; type = number default = 80 } Create a file outputs.tf in /modules/services/webserver-cluster directory: output \u0026#34;alb_dns_name\u0026#34; { value = aws_lb.example.dns_name description = \u0026#34;The domain name of the load balancer\u0026#34; } output \u0026#34;asg_name\u0026#34; { value = aws_autoscaling_group.example.name description = \u0026#34;The name of the Auto Scaling Group\u0026#34; } output \u0026#34;alb_security_group_id\u0026#34; { value = aws_security_group.alb.id description = \u0026#34;The ID of the Security Group attached to the load balancer\u0026#34; } Create a file main.tf in /prod/services/webserver-cluster directory terraform { required_version = \u0026#34;\u0026gt;= 1.0.0, \u0026lt; 2.0.0\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } module \u0026#34;webserver_cluster\u0026#34; { source = \u0026#34;../../../modules/services/webserver-cluster\u0026#34; cluster_name = var.cluster_name db_remote_state_bucket = var.db_remote_state_bucket db_remote_state_key = var.db_remote_state_key instance_type = \u0026#34;t2.micro\u0026#34; min_size = 2 max_size = 10 } resource \u0026#34;aws_autoscaling_schedule\u0026#34; \u0026#34;scale_out_during_business_hours\u0026#34; { scheduled_action_name = \u0026#34;scale-out-during-business-hours\u0026#34; min_size = 2 max_size = 10 desired_capacity = 10 recurrence = \u0026#34;0 9 * * *\u0026#34; autoscaling_group_name = module.webserver_cluster.asg_name } resource \u0026#34;aws_autoscaling_schedule\u0026#34; \u0026#34;scale_in_at_night\u0026#34; { scheduled_action_name = \u0026#34;scale-in-at-night\u0026#34; min_size = 2 max_size = 10 desired_capacity = 2 recurrence = \u0026#34;0 17 * * *\u0026#34; autoscaling_group_name = module.webserver_cluster.asg_name } Create a file variables.tf in /prod/services/webserver-cluster directory # --------------------------------------------------------------------------------------------------------------------- # REQUIRED PARAMETERS # You must provide a value for each of these parameters. # --------------------------------------------------------------------------------------------------------------------- variable \u0026#34;db_remote_state_bucket\u0026#34; { description = \u0026#34;The name of the S3 bucket used for the database\u0026#39;s remote state storage\u0026#34; type = string } variable \u0026#34;db_remote_state_key\u0026#34; { description = \u0026#34;The name of the key in the S3 bucket used for the database\u0026#39;s remote state storage\u0026#34; type = string } # --------------------------------------------------------------------------------------------------------------------- # OPTIONAL PARAMETERS # These parameters have reasonable defaults. # --------------------------------------------------------------------------------------------------------------------- variable \u0026#34;cluster_name\u0026#34; { description = \u0026#34;The name to use to namespace all the resources in the cluster\u0026#34; type = string default = \u0026#34;webservers-prod\u0026#34; } Create a file outputs.tf in /prod/services/webserver-cluster directory output \u0026#34;alb_dns_name\u0026#34; { value = module.webserver_cluster.alb_dns_name description = \u0026#34;The domain name of the load balancer\u0026#34; } Create a file main.tf in /prod/data-stores/mysql directory terraform { required_version = \u0026#34;\u0026gt;= 1.0.0, \u0026lt; 2.0.0\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; } } backend \u0026#34;s3\u0026#34; { # This backend configuration is filled in automatically at test time by Terratest. If you wish to run this example # manually, uncomment and fill in the config below. # bucket = \u0026#34;\u0026lt;YOUR S3 BUCKET\u0026gt;\u0026#34; # key = \u0026#34;\u0026lt;SOME PATH\u0026gt;/terraform.tfstate\u0026#34; # region = \u0026#34;us-east-1\u0026#34; # dynamodb_table = \u0026#34;\u0026lt;YOUR DYNAMODB TABLE\u0026gt;\u0026#34; # encrypt = true } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } resource \u0026#34;aws_db_instance\u0026#34; \u0026#34;example\u0026#34; { identifier_prefix = \u0026#34;terraform-up-and-running\u0026#34; engine = \u0026#34;mysql\u0026#34; allocated_storage = 10 instance_class = \u0026#34;db.t2.micro\u0026#34; db_name = var.db_name username = var.db_username password = var.db_password skip_final_snapshot = true } Create a file variables.tf in /prod/data-stores/mysql directory # --------------------------------------------------------------------------------------------------------------------- # REQUIRED PARAMETERS # You must provide a value for each of these parameters. # --------------------------------------------------------------------------------------------------------------------- variable \u0026#34;db_username\u0026#34; { description = \u0026#34;The username for the database\u0026#34; type = string sensitive = true } variable \u0026#34;db_password\u0026#34; { description = \u0026#34;The password for the database\u0026#34; type = string sensitive = true } # --------------------------------------------------------------------------------------------------------------------- # OPTIONAL PARAMETERS # These parameters have reasonable defaults. # --------------------------------------------------------------------------------------------------------------------- variable \u0026#34;db_name\u0026#34; { description = \u0026#34;The name to use for the database\u0026#34; type = string default = \u0026#34;example_database_prod\u0026#34; } Create a file outputs.tf in /prod/data-stores/mysql directory output \u0026#34;address\u0026#34; { value = aws_db_instance.example.address description = \u0026#34;Connect to the database at this endpoint\u0026#34; } output \u0026#34;port\u0026#34; { value = aws_db_instance.example.port description = \u0026#34;The port the database is listening on\u0026#34; } After writing the terraform code, you can run terraform fmt for reformat your configuration in the standard style.\n## Reformat your configuration in the standard style $ terraform fmt ## Prepare your working directory for other commands $ terraform init You need to run terraform init to initialize.\nRun terraform apply to create your infrastructure.\n## Create or update infrastructure $ terraform apply Enter \u0026ldquo;yes\u0026rdquo;.\nThis is the result after Terraform finishes running. You can check it on the console.\nEC2 Instance Auto Scaling Group Load Balancers Target Groups Check your website "
},
{
	"uri": "//localhost:1313/8-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "We will take the following this step to delete the resources we created in this exercise.\n## Destroy previously-created infrastructure terraform destroy Enter \u0026ldquo;Yes\u0026rdquo;.\nCheck EC2 Instance Go to EC2 service management console Check Auto Scaling Group Check Load Balancers Check Target Groups "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]